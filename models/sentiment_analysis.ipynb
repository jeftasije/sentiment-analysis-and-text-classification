{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text:str):\n",
    "    '''\n",
    "    This function removes english stopwords from input text.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        text : (str) Input text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): Filtered text without stopwords.\n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = text.split()\n",
    "    filtered_text = [word for word in words if (word not in stopwords_list or word in whitelist)]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def remove_breaklines(text:str):\n",
    "    '''\n",
    "    This function removes 'br' from input text.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        text : (str) Input text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): Filtered text without 'br'.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    filtered_text = [word.replace('br', '') for word in words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def clean_text(text: str):\n",
    "    '''\n",
    "    Cleans the input text by removing all punctuation, numbers, and special symbols,\n",
    "    leaving only alphabetic characters and whitespace.\n",
    "\n",
    "    This function uses a regular expression to replace all characters that are not\n",
    "    letters (a-z, A-Z) or whitespace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text (str): The input string to be cleaned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): The cleaned string with only alphabetic characters and whitespace.\n",
    "    '''\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def lemmatize(text: str):\n",
    "    '''\n",
    "    This functions lemmatizes the input text by reducing each word to its base form (lemma).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: (str) The input text to be lemmatized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): The lemmatized text where each word is reduced to its base form.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory, texts, labels):\n",
    "    '''\n",
    "    This function reads all text files in the given directory, appending the\n",
    "    content of each file to the `texts` list and the corresponding rating \n",
    "    (extracted from the file name) to the `labels` list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str): The path to the directory containing the text files.\n",
    "    labels (list): A list to which the ratings will be appended.\n",
    "    texts (list): A list to which the contents of the text files will be appended.\n",
    "\n",
    "    Each file in the directory should have a name in the format 'name_rating.txt'.\n",
    "    \n",
    "    There are two possible ratings:\n",
    "    - Negative (0): For ratings with a value of 4 or less.\n",
    "    - Positive (1): For ratings with a value of 5 or more.\n",
    "    '''\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            texts.append(file.read())\n",
    "        rating = file_name.split('_')[1].strip('.txt')\n",
    "        labels.append(0 if int(rating) <= 4 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "directory = '../datasets/imdb/train/pos/'\n",
    "load_data(directory, x_train, y_train)\n",
    "directory = '../datasets/imdb/train/neg/'\n",
    "load_data(directory, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [lemmatize(remove_stopwords(remove_breaklines(text))) for text in x_train]\n",
    "x_train = [clean_text(text) for text in x_train]\n",
    "\n",
    "output_size = np.unique(y_train).shape[0]\n",
    "y_train = to_categorical(y_train, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist()\n",
    "for text in x_train:\n",
    "    for word in text.split():\n",
    "        fdist[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = len([word for word in fdist if fdist[word] > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_of_words,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [len(text.split()) for text in x_train]\n",
    "max_length = max(seq_lengths)\n",
    "x_train_seq_seq = pad_sequences(x_train_seq, max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
